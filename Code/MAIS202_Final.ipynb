{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-15 09:40:13.933900: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-15 09:40:14.795637: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-11-15 09:40:14.967702: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-15 09:40:14.967735: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-11-15 09:40:15.115103: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-15 09:40:17.443328: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-15 09:40:17.443510: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-15 09:40:17.443521: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------------------\n",
    "# MAIS202 Group 1 - Fall 2022 Final Project\n",
    "# Jennifer Tram Su, Lucy Mao\n",
    "# Image Colourization\n",
    "#\n",
    "# Input : Single channel representation of greyscale image\n",
    "# Output : Two channel representation of colourized image\n",
    "#\n",
    "# https://algoritmaonline.com/image-colorization/\n",
    "# https://xiangyutang2.github.io/auto-colorization-autoencoders/\n",
    "# https://medium.com/@navmcgill/k-fold-cross-validation-in-keras-convolutional-neural-networks-835bed559d04\n",
    "# ----------------------------------------------------------------------------------------------\n",
    "\n",
    "# File Handling\n",
    "import glob\n",
    "from google.colab import files as f\n",
    "from google.colab import drive\n",
    "\n",
    "# Manipulation\n",
    "import numpy as np\n",
    "import json \n",
    "\n",
    "# Visualization\n",
    "from google.colab.patches import cv2_imshow\n",
    "from skimage.color import rgb2lab, lab2rgb\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Models\n",
    "import tensorflow as tf\n",
    "from keras.layers import Conv2D, UpSampling2D\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import UpSampling2D\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Concatenate\n",
    "from keras.utils import plot_model\n",
    "import numpy as np\n",
    "\n",
    "# Presentation\n",
    "from flask import Flask, flash, request, redirect, url_for, render_template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.upload()\n",
    "! mkdir /content/.kaggle\n",
    "! cp kaggle.json /content/.kaggle/\n",
    "!mv .kaggle /root/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets download -d hsankesara/flickr-image-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip /content/flickr-image-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drive.mount('/content/mydrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to dataset\n",
    "path = '/content/flickr30k_images/flickr30k_images/'\n",
    "extra = path + \"flickr30k_images\"\n",
    "!rm -rf extra\n",
    "\n",
    "files = glob.glob(path + \"*.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "def preprocess(img_list):\n",
    "    l_list = [0]*len(img_list)\n",
    "    ab_list = [0]*len(img_list)\n",
    "\n",
    "    for i in range(len(img_list)):\n",
    "      image = cv2.imread(img_list[i])\n",
    "\n",
    "      # Resize image\n",
    "      image = cv2.resize(image, (224, 224))\n",
    "\n",
    "      # Convert to LAB space\n",
    "      lab = rgb2lab(image)\n",
    "\n",
    "      # Extract L\n",
    "      l_list[i] = lab[:, :, 0].reshape((224, 224, 1))\n",
    "#      l_list[i] = np.stack((l_list[i],)*3, axis=2).reshape((224, 224, 3))\n",
    "\n",
    "      ab_list[i] = lab[:, :, 1:]\n",
    "\n",
    "    l_list = np.array(l_list)\n",
    "    ab_list = np.array(ab_list)\n",
    "\n",
    "    return l_list, ab_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(l, ab):\n",
    "  l = l.reshape((224, 224, 1))\n",
    "  \n",
    "  a =  ab[:, :, 0].reshape((224, 224, 1))\n",
    "  b = ab[:, :, 1].reshape((224, 224, 1))\n",
    "\n",
    "  outimg = np.stack((l, a, b), axis=2)\n",
    "\n",
    "  outimg = outimg.reshape((224, 224, 3))\n",
    "\n",
    "  r = lab2rgb(outimg)*255\n",
    "  cv2_imshow(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CNN(Model):\n",
    "\n",
    "  def __init__(self, path = None):\n",
    "    super().__init__()\n",
    "    if path: # Load saved model\n",
    "      self.model = tf.keras.models.load_model(path)\n",
    "    else: # Training new model\n",
    "      self.model_input = Input(shape=(224, 224, 1,))\n",
    "\n",
    "      # Feature extractor model\n",
    "      self.extractor_input = Input(shape=(4096,))\n",
    "      self.image_feature = Dropout(0.5)(self.extractor_input)\n",
    "      self.image_feature = Dense(1024, activation='relu')(self.image_feature)\n",
    "\n",
    "      encoder_output = Conv2D(64, (3,3), activation='relu', padding='same', strides=2)(self.model_input)\n",
    "      encoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "      encoder_output = Conv2D(128, (3,3), activation='relu', padding='same', strides=2)(encoder_output)\n",
    "      encoder_output = Conv2D(256, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "      encoder_output = Conv2D(256, (3,3), activation='relu', padding='same', strides=2)(encoder_output)\n",
    "      encoder_output = Conv2D(512, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "      encoder_output = Conv2D(512, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "      encoder_output = Conv2D(256, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "\n",
    "      # concat_shape = (np.uint32(encoder_output.shape[1]), np.uint32(encoder_output.shape[2]),np.uint32(self.extractor_input.shape[-1]))\n",
    "\n",
    "      # image_feature = RepeatVector(int(concat_shape[0]*concat_shape[1]))(self.extractor_input)\n",
    "      # image_feature = Reshape(concat_shape)(image_feature)\n",
    "\n",
    "      # fusion_output = Concatenate(axis=3)([encoder_output, image_feature])\n",
    "    \n",
    "      decoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "      decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
    "      decoder_output = Conv2D(64, (3,3), activation='relu', padding='same')(decoder_output)\n",
    "      decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
    "      decoder_output = Conv2D(32, (3,3), activation='relu', padding='same')(decoder_output)\n",
    "      decoder_output = Conv2D(16, (3,3), activation='relu', padding='same')(decoder_output)\n",
    "      decoder_output = Conv2D(2, (3, 3), activation='tanh', padding='same')(decoder_output)\n",
    "      decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
    "\n",
    "      self.model = Model(inputs=[self.model_input], outputs=decoder_output)\n",
    "      self.model.compile(loss='mean_squared_error', optimizer='adam', metrics=['acc'])\n",
    "      # summarize model\n",
    "      #print(self.model.summary())\n",
    "      #plot_model(model, to_file='autoencoder_colorization_merged.png', show_shapes=True)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATES = [0.0001, 0.001, 0.01]\n",
    "\n",
    "train = files[:30000]\n",
    "test = files[30000:]\n",
    "\n",
    "BATCH_SIZE = 50 # each epoch takes 4 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training model...\")\n",
    "\n",
    "for alpha in LEARNING_RATES:\n",
    "  print(f\"Training for learning rate {alpha}...\")\n",
    "\n",
    "  alpha_path = f\"/content/mydrive/MyDrive/models/LEARNING_RATES/{alpha}\"\n",
    "  ! mkdir {alpha_path}\n",
    "\n",
    "  for i in range(150):\n",
    "    print(f\"Starting batch {i} of 150...\")\n",
    "    model_path = alpha_path + f\"/{i}\"\n",
    "\n",
    "    if i == 0:\n",
    "      cnn = CNN().model\n",
    "    elif (i-1) % 30 == 0:\n",
    "      # Load previous model\n",
    "      prev_path = alpha_path + f\"/{i-1}\"\n",
    "      cnn = CNN(path = prev_path).model\n",
    "\n",
    "    l_files, ab_files = preprocess(train[i*200:(i+1)*200])\n",
    "\n",
    "    X = l_files\n",
    "    y = ab_files\n",
    "\n",
    "    cnn.fit(x=X,y=y,validation_split=0.1, epochs=5, batch_size=BATCH_SIZE, verbose=0, use_multiprocessing=True, workers=16)\n",
    "\n",
    "    # Save metrics\n",
    "    history = cnn.history.history\n",
    "    print(history)\n",
    "\n",
    "    with open(model_path + f\"_metrics.txt\", 'w') as f:\n",
    "      f.write(json.dumps(history))\n",
    "      \n",
    "    # Save model\n",
    "    if i % 30 == 0:\n",
    "      cnn.save(model_path)\n",
    "\n",
    "  # Out of loop, save final model\n",
    "  cnn.save(alpha_path + \"/FINAL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing FINAL model\n",
    "\n",
    "cnnmodel = CNN(\"/content/mydrive/MyDrive/models/LEARNING_RATES/0.0001/FINAL\").model\n",
    "\n",
    "l, ab = preprocess([test[10]])\n",
    "\n",
    "out1 = cnnmodel.predict(x=l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(l[0], ab[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(l[0], out1[0]*128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing FINAL model\n",
    "\n",
    "cnnmodel = CNN(\"/content/mydrive/MyDrive/models/LEARNING_RATES/0.001/FINAL\").model\n",
    "\n",
    "l, ab = preprocess([test[10]])\n",
    "\n",
    "out2 = cnnmodel.predict(x=l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(l[0], out2[0]*128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing FINAL model\n",
    "\n",
    "cnnmodel = CNN(\"/content/mydrive/MyDrive/models/LEARNING_RATES/0.01/FINAL\").model\n",
    "\n",
    "l, ab = preprocess([test[10]])\n",
    "\n",
    "out3 = cnnmodel.predict(x=l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(l[0], out3[0]*128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print metrics for each model\n",
    "\n",
    "losses = {}\n",
    "acc = {}\n",
    "val_loss = {}\n",
    "val_acc = {}\n",
    "\n",
    "for alpha in LEARNING_RATES:\n",
    "  alpha_path = f\"/content/mydrive/MyDrive/models/LEARNING_RATES/{alpha}\"\n",
    "\n",
    "  text_paths = glob.glob(alpha_path + \"/*.txt\")\n",
    "\n",
    "  l = []\n",
    "  a = []\n",
    "  ll = []\n",
    "  la = []\n",
    "\n",
    "  for text in text_paths:\n",
    "\n",
    "    with open(text) as f:\n",
    "      data = json.load(f)\n",
    "\n",
    "      l.extend(data['loss'])\n",
    "      a.extend(data['acc'])\n",
    "      ll.extend(data['val_loss'])\n",
    "      la.extend(data['val_acc'])\n",
    "\n",
    "    losses[alpha] = l\n",
    "    acc[alpha] = a\n",
    "    val_loss[alpha] = ll\n",
    "    val_acc[alpha] = la\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(loss, acc, val_loss, val_acc, title):\n",
    "\n",
    "  epochs = np.arange(5*150)\n",
    "\n",
    "  plt.figure(figsize=(10, 15))\n",
    "  plt.subplot(2, 1, 1)\n",
    "  plt.plot(epochs, loss)\n",
    "  plt.plot(epochs, val_loss)\n",
    "  plt.legend(['Train', 'Validation'])\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel(\"Loss\")\n",
    "  plt.title(title)\n",
    "\n",
    "  plt.subplot(2, 1, 2)\n",
    "  plt.plot(epochs, acc)\n",
    "  plt.plot(epochs, val_acc)\n",
    "  plt.legend(['Train', 'Validation'])\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel(\"Accuracy\")\n",
    "\n",
    "  plt.show()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(losses[0.0001], acc[0.0001], val_loss[0.0001], val_acc[0.0001], \"Learning Rate = 0.0001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(losses[0.001], acc[0.001], val_loss[0.001], val_acc[0.001], \"Learning Rate = 0.001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(losses[0.01], acc[0.01], val_loss[0.01], val_acc[0.01], \"Learning Rate = 0.01\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('mais')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0111e3fba090b038a7123b9e23f8d01ae43d39ccee8f8d8074f3f1c124641d1f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
